{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Jeg tror det vil være dejlig køligt, men jeg v...\n",
       "1       Så kommer de nok til at investere i en ny cyke...\n",
       "2       Nu er det jo også de Ikea-aber der har lavet s...\n",
       "3       128 Varme emails, er vi enige om at det er sex...\n",
       "4       Desværre tyder det på, at amerikanerne er helt...\n",
       "                              ...                        \n",
       "2955    Har sgu lidt en anelse om... det her kunne mås...\n",
       "2956    Ind og ruske tremmer med hende,Den syge kælling!!\n",
       "2957                                             fedtmule\n",
       "2958                                    ##HAR I HØRT DET?\n",
       "2959    Kommer det bag på nogen? Det er jo fucking var...\n",
       "Name: tweet, Length: 2960, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Danish/Danish/offenseval-da-training-v1.tsv\", sep='\\t')[:-1]\n",
    "\n",
    "texts = data[\"tweet\"].apply(str)\n",
    "labels = data[\"subtask_a\"].apply(str)\n",
    "#print(data.loc[2883])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@USER',\n",
       " 'næste',\n",
       " 'gang',\n",
       " 'pastaen',\n",
       " 'er',\n",
       " 'brændt',\n",
       " 'på',\n",
       " '!',\n",
       " ':face_with_tears_of_joy:',\n",
       " 'det',\n",
       " 'jo',\n",
       " 'sygt...']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "texts_tokenized = []\n",
    "for el in texts:\n",
    "    texts_tokenized.append([emoji.demojize(el) for el in el.split()]) #nltk.word_tokenize(el)\n",
    "\n",
    "    \n",
    "texts_tokenized[2883]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@user', 'næste', 'gang', 'pastaen', 'er', 'brændt', 'på', 'det', 'jo', 'sygt']\n"
     ]
    }
   ],
   "source": [
    "#normalization\n",
    "texts_normalized = []\n",
    "for sent in texts_tokenized:\n",
    "    # remove emojis\n",
    "    sent = [el.lower() for el in sent if not bool(re.search(r\":.*:\", el))] #or replace with just name?\n",
    "    # remove repeating characters\n",
    "    sent = [re.sub(r\"(.)\\1{2,}\", r\"\\1\", el) for el in sent]\n",
    "    sent = [re.sub(r\"[!?\\(\\)\\\\&.,:><_#\\[\\]/]+\", \"\", el)for el in sent]\n",
    "    texts_normalized.append([el for el in sent if el != \"\"])\n",
    "        \n",
    "print(texts_normalized[2883])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3131</td>\n",
       "      <td>jeg tror det vil være dejlig køligt men jeg vi...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711</td>\n",
       "      <td>så kommer de nok til at investere i en ny cyke...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2500</td>\n",
       "      <td>nu er det jo også de ikea-aber der har lavet s...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2678</td>\n",
       "      <td>128 varme emails er vi enige om at det er sext...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>desværre tyder det på at amerikanerne er helt ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>170</td>\n",
       "      <td>har sgu lidt en anelse om det her kunne måske ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>1226</td>\n",
       "      <td>ind og ruske tremmer med hendeden syge kælling</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>2596</td>\n",
       "      <td>fedtmule</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>1802</td>\n",
       "      <td>har i hørt det</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>2809</td>\n",
       "      <td>kommer det bag på nogen det er jo fucking varm...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2960 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet subtask_a\n",
       "0     3131  jeg tror det vil være dejlig køligt men jeg vi...       NOT\n",
       "1      711  så kommer de nok til at investere i en ny cyke...       NOT\n",
       "2     2500  nu er det jo også de ikea-aber der har lavet s...       OFF\n",
       "3     2678  128 varme emails er vi enige om at det er sext...       NOT\n",
       "4      784  desværre tyder det på at amerikanerne er helt ...       NOT\n",
       "...    ...                                                ...       ...\n",
       "2955   170  har sgu lidt en anelse om det her kunne måske ...       NOT\n",
       "2956  1226     ind og ruske tremmer med hendeden syge kælling       OFF\n",
       "2957  2596                                           fedtmule       NOT\n",
       "2958  1802                                     har i hørt det       NOT\n",
       "2959  2809  kommer det bag på nogen det er jo fucking varm...       OFF\n",
       "\n",
       "[2960 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_normalized2 = [\" \".join(el) for el in texts_normalized]\n",
    "texts_normalized3 = pd.DataFrame(texts_normalized2, columns=[\"tweet\"])\n",
    "texts_normalized3\n",
    "data[\"tweet\"] = texts_normalized2\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0       jeg tror det vil være dejlig køligt men jeg vi...\n",
       " 1       så kommer de nok til at investere i en ny cyke...\n",
       " 2       nu er det jo også de ikea-aber der har lavet s...\n",
       " 3       128 varme emails er vi enige om at det er sext...\n",
       " 4       desværre tyder det på at amerikanerne er helt ...\n",
       "                               ...                        \n",
       " 2955    har sgu lidt en anelse om det her kunne måske ...\n",
       " 2956       ind og ruske tremmer med hendeden syge kælling\n",
       " 2957                                             fedtmule\n",
       " 2958                                       har i hørt det\n",
       " 2959    kommer det bag på nogen det er jo fucking varm...\n",
       " Name: tweet, Length: 2960, dtype: object,\n",
       " 0       NOT\n",
       " 1       NOT\n",
       " 2       OFF\n",
       " 3       NOT\n",
       " 4       NOT\n",
       "        ... \n",
       " 2955    NOT\n",
       " 2956    OFF\n",
       " 2957    NOT\n",
       " 2958    NOT\n",
       " 2959    OFF\n",
       " Name: subtask_a, Length: 2960, dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data[\"tweet\"].apply(str)\n",
    "texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2734    jeg vil foreslå to andre billeder af københavn...\n",
       " 2886    og der bliver stadigt flereurl danmark længe leve\n",
       " 2738                                                  lol\n",
       " 2937    ah og vi har allerede fået bygget en mur mod i...\n",
       " 2671    vi må da sige han går da all in når man skal t...\n",
       "                               ...                        \n",
       " 1147    det er jo ikke sjovt nogen ting spøger man bar...\n",
       " 2154                                  det har du os rat i\n",
       " 1766                          grønt er altså for bornholm\n",
       " 1122    de skal bare kvæles langsomt så de ligger stil...\n",
       " 1346    syntes man begynder at høre mere om partering ...\n",
       " Name: tweet, Length: 2220, dtype: object,\n",
       " 418     ikke noget andre kvinder ikke har men ser bedr...\n",
       " 1224    foretrækker dog isterningebakken den kan bruge...\n",
       " 1617    troede at vores flag altid brandte det gør det...\n",
       " 1745                                                 mere\n",
       " 2139                        også kendt som opium med brus\n",
       "                               ...                        \n",
       " 2529          fantastisk at du gad lave al den research d\n",
       " 2883    @user næste gang pastaen er brændt på det jo sygt\n",
       " 1678    ”vi kan jo ikke faktatjekke alting og når det ...\n",
       " 375                      jeg gad godt have det der pr job\n",
       " 219     jeg leger lige djævelens advokat jantelov er h...\n",
       " Name: tweet, Length: 740, dtype: object,\n",
       " 2734    NOT\n",
       " 2886    NOT\n",
       " 2738    NOT\n",
       " 2937    NOT\n",
       " 2671    NOT\n",
       "        ... \n",
       " 1147    NOT\n",
       " 2154    NOT\n",
       " 1766    NOT\n",
       " 1122    NOT\n",
       " 1346    NOT\n",
       " Name: subtask_a, Length: 2220, dtype: object,\n",
       " 418     NOT\n",
       " 1224    NOT\n",
       " 1617    NOT\n",
       " 1745    NOT\n",
       " 2139    NOT\n",
       "        ... \n",
       " 2529    NOT\n",
       " 2883    NOT\n",
       " 1678    OFF\n",
       " 375     NOT\n",
       " 219     NOT\n",
       " Name: subtask_a, Length: 740, dtype: object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.25, random_state=123)\n",
    "texts_train, texts_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT    2576\n",
       "OFF     384\n",
       "Name: subtask_a, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subtask_a'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT' 'OFF']\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "le =LabelEncoder()\n",
    "le.fit(labels_train)\n",
    "print(le.classes_)\n",
    "train_labels=le.transform(labels_train)\n",
    "print(train_labels)\n",
    "test_labels=le.transform(labels_test)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(texts_train)\n",
    "#print(texts_train)\n",
    "train_texts = vectorizer.transform(texts_train)\n",
    "test_texts = vectorizer.transform(texts_test)\n",
    "print(train_texts.toarray())\n",
    "print(test_texts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8127"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(train_texts.toarray()[0])\n",
    "input_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        '''\n",
    "        Defining layers of neural network\n",
    "        '''\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (data_points, num_features)\n",
    "        Returns:\n",
    "            the resulting tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        #print(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        #print(x)\n",
    "        x = self.fc2(x)\n",
    "        #print(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        #x = torch.sigmoid(self.fc2(torch.sigmoid(self.fc1(x))))\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "net = ClassificationNet()\n",
    "\n",
    "# learning rate\n",
    "\n",
    "n = 0.05\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=n)\n",
    "\n",
    "# loss function\n",
    "loss_func = nn.CrossEntropyLoss() #nn.BCELoss()\n",
    "\n",
    "\n",
    "epochs= 500 \n",
    "\n",
    "#converting train and test set arrays to tensor\n",
    "train_texts_tensor=torch.tensor(train_texts.toarray()).float()\n",
    "print(len(train_texts_tensor))\n",
    "train_labels_tensor=torch.tensor(train_labels)\n",
    "test_texts_tensor=torch.tensor(test_texts.toarray()).float()\n",
    "test_labels_tensor=torch.tensor(test_labels)\n",
    "print(test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(pred_label,true_label):\n",
    "    '''\n",
    "    Returns accuracy and f1 score metrics for evaluation\n",
    "    '''\n",
    "    accuracy=accuracy_score(true_label,pred_label)\n",
    "    f1score=f1_score(true_label,pred_label,average='macro')\n",
    "    \n",
    "    return (accuracy,f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.904054054054054 0.47480482611781405\n",
      "10 0.904054054054054 0.47480482611781405\n",
      "20 0.904054054054054 0.47480482611781405\n",
      "30 0.904054054054054 0.5363531270131223\n",
      "40 0.9 0.4736842105263158\n",
      "50 0.9 0.48680461838356576\n",
      "60 0.904054054054054 0.546877560348768\n",
      "70 0.904054054054054 0.47480482611781405\n",
      "80 0.904054054054054 0.47480482611781405\n",
      "90 0.904054054054054 0.47480482611781405\n",
      "100 0.904054054054054 0.47480482611781405\n",
      "110 0.904054054054054 0.47480482611781405\n",
      "120 0.904054054054054 0.47480482611781405\n",
      "130 0.904054054054054 0.47480482611781405\n",
      "140 0.904054054054054 0.47480482611781405\n",
      "150 0.904054054054054 0.47480482611781405\n",
      "160 0.9027027027027027 0.4744318181818182\n",
      "170 0.904054054054054 0.47480482611781405\n",
      "180 0.9013513513513514 0.5542573293396373\n",
      "190 0.8878378378378379 0.5819237497532519\n",
      "200 0.9067567567567567 0.5154448398576513\n",
      "210 0.9067567567567567 0.5494136023085272\n",
      "220 0.8783783783783784 0.615153476331361\n",
      "230 0.9 0.6304394773782529\n",
      "240 0.9148648648648648 0.6311679681010134\n",
      "250 0.9162162162162162 0.6546844894026975\n",
      "260 0.8918918918918919 0.6996996996996997\n",
      "270 0.9067567567567567 0.7187938868236265\n",
      "280 0.927027027027027 0.7157005037138222\n",
      "290 0.927027027027027 0.7104347826086957\n",
      "300 0.9162162162162162 0.7385754985754986\n",
      "310 0.9027027027027027 0.7195730436430241\n",
      "320 0.9202702702702703 0.7173579506832998\n",
      "330 0.922972972972973 0.7269390371008151\n",
      "340 0.8932432432432432 0.694232469101579\n",
      "350 0.8905405405405405 0.6825195048702588\n",
      "360 0.9216216216216216 0.7198725981620718\n",
      "370 0.9216216216216216 0.7151957531519576\n",
      "380 0.9013513513513514 0.6943169664835134\n",
      "390 0.8824324324324324 0.6791891528261552\n",
      "400 0.8986486486486487 0.6983548108331386\n",
      "410 0.922972972972973 0.7269390371008151\n",
      "420 0.927027027027027 0.7348374253483743\n",
      "430 0.9256756756756757 0.7365201235183303\n",
      "440 0.9175675675675675 0.7295564702146653\n",
      "450 0.918918918918919 0.7320070499046331\n",
      "460 0.9202702702702703 0.7173579506832998\n",
      "470 0.9202702702702703 0.7218665392578436\n",
      "480 0.9175675675675675 0.7169391299985578\n",
      "490 0.918918918918919 0.7320070499046331\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for i in range(epochs):\n",
    "    \n",
    "    #optimizer.zero_grad()\n",
    "    output = net(train_texts_tensor)\n",
    "    #print(output)\n",
    "    #print(output.shape, train_labels_tensor.shape)\n",
    "    loss = loss_func(output, train_labels_tensor.long())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            \n",
    "        output = net(test_texts_tensor)\n",
    "            \n",
    "        loss_val = loss_func(output, test_labels_tensor.long())\n",
    "        \n",
    "        predict_label= output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        accuracy,f1score=evaluation_metrics(predict_label,test_labels_tensor.long())\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print(i, accuracy, f1score)\n",
    "#for el in zip(predict_label, test_labels):\n",
    "    #print(el)\n",
    "        #pred = torch.stack(predict_y, 1).squeeze()\n",
    "        #print(pred)\n",
    "        #ev = evaluation_metrics(pred, test_labels_tensor.float())\n",
    "        #print(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  7.2928,  -7.3610],\n",
      "        [ 12.8236, -12.8625],\n",
      "        [  2.7378,  -2.8031],\n",
      "        ...,\n",
      "        [ -1.9575,   1.9652],\n",
      "        [  6.8403,  -6.8913],\n",
      "        [  4.2639,  -4.2754]], grad_fn=<AddmmBackward>)\n",
      "678 62 740\n"
     ]
    }
   ],
   "source": [
    "output = net(test_texts_tensor)\n",
    "print(output)\n",
    "predict_label= output.data.max(1, keepdim=True)[1]\n",
    "tp = 0\n",
    "all = 0\n",
    "f = 0\n",
    "for el, e in zip(predict_label, test_labels):\n",
    "    if el.item() == e:\n",
    "        tp += 1\n",
    "    else:\n",
    "        f += 1\n",
    "    all += 1\n",
    "print(tp, f, all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
