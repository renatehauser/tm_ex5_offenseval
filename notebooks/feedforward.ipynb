{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../Danish/Danish/offenseval-da-training-v1.tsv\", sep='\\t')[:-1]\n",
    "\n",
    "texts = data[\"tweet\"].apply(str)\n",
    "labels = data[\"subtask_a\"].apply(str)\n",
    "#texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and replacing emojis\n",
    "texts_tokenized = []\n",
    "for el in texts:\n",
    "    texts_tokenized.append([emoji.demojize(el).lower() for el in el.split()]) #el.split()nltk.word_tokenize(el)\n",
    "    \n",
    "#texts_tokenized[2883]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@user', 'næste', 'gang', 'pastaen', 'er', 'brændt', 'på', '!', ':face_with_tears_of_joy:', 'det', 'jo', 'sygt.']\n"
     ]
    }
   ],
   "source": [
    "#normalization\n",
    "texts_normalized = []\n",
    "for sent in texts_tokenized:\n",
    "    # remove emojis\n",
    "    #sent = [el for el in sent if not bool(re.search(r\":.*:\", el))] #or replace with just name? \n",
    "    \n",
    "    # remove repeating characters\n",
    "    sent = [re.sub(r\"(.)\\1{2,}\", r\"\\1\", el) for el in sent]\n",
    "    \n",
    "    # remove punctuation\n",
    "    #sent = [re.sub(r\"[!?\\(\\)\\\\&.,:><_#\\[\\]/]+\", \"\", el)for el in sent]\n",
    "    \n",
    "    texts_normalized.append([el for el in sent if el != \"\"])\n",
    "        \n",
    "print(texts_normalized[2883])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3131</td>\n",
       "      <td>jeg tror det vil være dejlig køligt, men jeg v...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711</td>\n",
       "      <td>så kommer de nok til at investere i en ny cyke...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2500</td>\n",
       "      <td>nu er det jo også de ikea-aber der har lavet s...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2678</td>\n",
       "      <td>128 varme emails, er vi enige om at det er sex...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>desværre tyder det på, at amerikanerne er helt...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>170</td>\n",
       "      <td>har sgu lidt en anelse om. det her kunne måske...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>1226</td>\n",
       "      <td>ind og ruske tremmer med hende,den syge kælling!!</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>2596</td>\n",
       "      <td>fedtmule</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>1802</td>\n",
       "      <td>##har i hørt det?</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>2809</td>\n",
       "      <td>kommer det bag på nogen? det er jo fucking var...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2960 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet subtask_a\n",
       "0     3131  jeg tror det vil være dejlig køligt, men jeg v...       NOT\n",
       "1      711  så kommer de nok til at investere i en ny cyke...       NOT\n",
       "2     2500  nu er det jo også de ikea-aber der har lavet s...       OFF\n",
       "3     2678  128 varme emails, er vi enige om at det er sex...       NOT\n",
       "4      784  desværre tyder det på, at amerikanerne er helt...       NOT\n",
       "...    ...                                                ...       ...\n",
       "2955   170  har sgu lidt en anelse om. det her kunne måske...       NOT\n",
       "2956  1226  ind og ruske tremmer med hende,den syge kælling!!       OFF\n",
       "2957  2596                                           fedtmule       NOT\n",
       "2958  1802                                  ##har i hørt det?       NOT\n",
       "2959  2809  kommer det bag på nogen? det er jo fucking var...       OFF\n",
       "\n",
       "[2960 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the preprocessed texts back into the dataframe\n",
    "texts_normalized2 = [\" \".join(el) for el in texts_normalized]\n",
    "data[\"tweet\"] = texts_normalized2\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0       jeg tror det vil være dejlig køligt, men jeg v...\n",
       " 1       så kommer de nok til at investere i en ny cyke...\n",
       " 2       nu er det jo også de ikea-aber der har lavet s...\n",
       " 3       128 varme emails, er vi enige om at det er sex...\n",
       " 4       desværre tyder det på, at amerikanerne er helt...\n",
       "                               ...                        \n",
       " 2955    har sgu lidt en anelse om. det her kunne måske...\n",
       " 2956    ind og ruske tremmer med hende,den syge kælling!!\n",
       " 2957                                             fedtmule\n",
       " 2958                                    ##har i hørt det?\n",
       " 2959    kommer det bag på nogen? det er jo fucking var...\n",
       " Name: tweet, Length: 2960, dtype: object,\n",
       " 0       NOT\n",
       " 1       NOT\n",
       " 2       OFF\n",
       " 3       NOT\n",
       " 4       NOT\n",
       "        ... \n",
       " 2955    NOT\n",
       " 2956    OFF\n",
       " 2957    NOT\n",
       " 2958    NOT\n",
       " 2959    OFF\n",
       " Name: subtask_a, Length: 2960, dtype: object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the tweets and labels\n",
    "texts = data[\"tweet\"].apply(str)\n",
    "texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(711                                              not bad!\n",
       " 1412    ragnerok nærmer sig? jeg har svært ved at se f...\n",
       " 1779                  *långyxa* på riktigt välfärdsspråk.\n",
       " 773     hrmph. lånte fjer! er intet for helligt for ka...\n",
       " 1286    det er sikkert svensken der er skyld i der ald...\n",
       "                               ...                        \n",
       " 1147    det er jo ikke sjovt! nogen ting spøger man ba...\n",
       " 2154                                  det har du os rat i\n",
       " 1766                          grønt er altså for bornholm\n",
       " 1122    de skal bare kvæles langsomt så de ligger stil...\n",
       " 1346    syntes man begynder at høre mere om partering ...\n",
       " Name: tweet, Length: 2664, dtype: object,\n",
       " 418     ikke noget andre kvinder ikke har men ser bedr...\n",
       " 1224    foretrækker dog isterningebakken, den kan brug...\n",
       " 1617    troede at vores flag altid brandte?.. det gør ...\n",
       " 1745                                                mere?\n",
       " 2139                      også kendt, som opium med brus.\n",
       "                               ...                        \n",
       " 2135                                  tak, op. mange tak.\n",
       " 2364                                           ew, sweden\n",
       " 518       tak for et lyspunkt i denne ellers så mørke tid\n",
       " 1979                  smid denne over på /r/denmark også!\n",
       " 1529        @user du er også lidt derhen af med din pasta\n",
       " Name: tweet, Length: 296, dtype: object,\n",
       " 711     NOT\n",
       " 1412    NOT\n",
       " 1779    NOT\n",
       " 773     NOT\n",
       " 1286    NOT\n",
       "        ... \n",
       " 1147    NOT\n",
       " 2154    NOT\n",
       " 1766    NOT\n",
       " 1122    NOT\n",
       " 1346    NOT\n",
       " Name: subtask_a, Length: 2664, dtype: object,\n",
       " 418     NOT\n",
       " 1224    NOT\n",
       " 1617    NOT\n",
       " 1745    NOT\n",
       " 2139    NOT\n",
       "        ... \n",
       " 2135    NOT\n",
       " 2364    OFF\n",
       " 518     NOT\n",
       " 1979    NOT\n",
       " 1529    NOT\n",
       " Name: subtask_a, Length: 296, dtype: object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.1, random_state=123)\n",
    "texts_train, texts_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT    2576\n",
       "OFF     384\n",
       "Name: subtask_a, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subtask_a'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT' 'OFF']\n"
     ]
    }
   ],
   "source": [
    "le =LabelEncoder()\n",
    "le.fit(labels_train)\n",
    "print(le.classes_)\n",
    "train_labels=le.transform(labels_train)\n",
    "#print(train_labels)\n",
    "test_labels=le.transform(labels_test)\n",
    "#print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() #CountVectorizer()\n",
    "vectorizer.fit_transform(texts)\n",
    "#print(texts_train)\n",
    "train_texts = vectorizer.transform(texts_train)\n",
    "test_texts = vectorizer.transform(texts_test)\n",
    "#print(train_texts.toarray())\n",
    "#print(test_texts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9738"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(train_texts.toarray()[0])\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple FFN with a linear layer and the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        '''\n",
    "        Defining layers of neural network\n",
    "        '''\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (data_points, num_features)\n",
    "        Returns:\n",
    "            the resulting tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664\n"
     ]
    }
   ],
   "source": [
    "net = ClassificationNet()\n",
    "\n",
    "# define learning rate\n",
    "\n",
    "n = 0.05\n",
    "\n",
    "# define an optimizer\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=n)\n",
    "\n",
    "# define loss function\n",
    "#loss_func = nn.BCELoss() #nn.CrossEntropyLoss() \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# define number of epochs\n",
    "epochs= 500 \n",
    "\n",
    "#converting train and test set arrays to tensor\n",
    "train_texts_tensor=torch.tensor(train_texts.toarray()).float()\n",
    "print(len(train_texts_tensor))\n",
    "train_labels_tensor=torch.tensor(train_labels)\n",
    "test_texts_tensor=torch.tensor(test_texts.toarray()).float()\n",
    "test_labels_tensor=torch.tensor(test_labels)\n",
    "#print(train_texts_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(pred_label,true_label):\n",
    "    '''\n",
    "    Returns accuracy and f1 score metrics for evaluation\n",
    "    '''\n",
    "    accuracy=accuracy_score(true_label,pred_label)\n",
    "    f1score=f1_score(true_label,pred_label,average='macro')\n",
    "    precision= precision_score(true_label, pred_label,average='macro')\n",
    "    recall=recall_score(true_label, pred_label,average='macro')\n",
    "    \n",
    "    return f\"Acc: {accuracy}, Prec: {precision}, Rec: {recall}, F1: {f1score}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\glase\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "10 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "20 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "30 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "40 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "50 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "60 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "70 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "80 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "90 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "100 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "110 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "120 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "130 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "140 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "150 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "160 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "170 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "180 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "190 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "200 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "210 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "220 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "230 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n",
      "240 Acc: 0.9054054054054054, Prec: 0.4527027027027027, Rec: 0.5, F1: 0.475177304964539\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-103898a13740>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(output, train_labels_tensor.unsqueeze(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(output.shape, train_labels_tensor.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-b51fa943c715>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mresulting\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \"\"\"\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1672\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = net(train_texts_tensor)\n",
    "    #print(output, train_labels_tensor.unsqueeze(1))\n",
    "    #print(output.shape, train_labels_tensor.shape)\n",
    "    loss = loss_func(output, train_labels_tensor.long()) #float().unsqueeze(1)\n",
    "    #loss = loss_func(output, train_labels_tensor.float().unsqueeze(1))\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "            \n",
    "        output = net(test_texts_tensor)\n",
    "            \n",
    "        loss_val = loss_func(output, test_labels_tensor.long()) #)float()).unsqueeze(1)\n",
    "        #loss_val = loss_func(output,test_labels_tensor.float().unsqueeze(1))\n",
    "        \n",
    "        predict_label= output.data.max(1, keepdim=True)[1]\n",
    "        #predict_label=[step(el) for el in output]\n",
    "        #print(predict_label)\n",
    "        \n",
    "        #accuracy,f1score=evaluation_metrics(predict_label,test_labels_tensor.long())\n",
    "        eval_metrics=evaluation_metrics(predict_label,test_labels_tensor.long())\n",
    "        \n",
    "    if i%10 == 0:\n",
    "        print(i, eval_metrics)\n",
    "print(i, eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268 28 28 296\n"
     ]
    }
   ],
   "source": [
    "# count how many tweets get labeled correctly, how many are labeled wrongly, the amount of off tweets in the test set\n",
    "# and the amount of tweets in total in the test set\n",
    "output = net(test_texts_tensor)\n",
    "predict_label= output.data.max(1, keepdim=True)[1]\n",
    "#predict_label=[step(el) for el in output]\n",
    "#print(output)\n",
    "tp = 0\n",
    "all_off = 0\n",
    "all = 0\n",
    "f = 0\n",
    "for el, e in zip(predict_label, test_labels):\n",
    "    #print(el.item(), e)\n",
    "    if e == 1:\n",
    "        all_off += 1\n",
    "    if el.item() == e:\n",
    "        tp += 1\n",
    "    else:\n",
    "        f += 1\n",
    "    all += 1\n",
    "print(tp, f, all_off, all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
